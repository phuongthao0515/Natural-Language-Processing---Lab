{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFD7DVZ-xKdT"
      },
      "source": [
        "# Homework Lab 2: Text Preprocessing with Vietnamese\n",
        "**Overview:** In this exercise, we will build a text preprocessing program for Vietnamese."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOAeiqdrxKdt"
      },
      "source": [
        "Import the necessary libraries. Note that we are using the underthesea library for Vietnamese tokenization. To install it, follow the instructions below. ([link](https://github.com/undertheseanlp/underthesea))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "hiheCNPuP2Es"
      },
      "outputs": [],
      "source": [
        "# !pip install underthesea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "RrFQ_Ht_xKdu"
      },
      "outputs": [],
      "source": [
        "import os,glob\n",
        "import codecs\n",
        "import sys\n",
        "import re\n",
        "from underthesea import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC27lBQZxKdw"
      },
      "source": [
        "## Question 1: Create a Corpus and Survey the Data\n",
        "\n",
        "The data in this section is partially extracted from the [VNTC](https://github.com/duyvuleo/VNTC) dataset. VNTC is a Vietnamese news dataset covering various topics. In this section, we will only process the science topic from VNTC. We will create a corpus from both the train and test directories. Complete the following program:\n",
        "\n",
        "- Write `sentences_list` to a file named `dataset_name.txt`, with each element as a document on a separate line.\n",
        "- Check how many documents are in the corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NApbySbhYukC",
        "outputId": "484d5aa1-bbb8-4657-f4cd-a663e2926e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyNKT8wAxKdx",
        "outputId": "de8b59b1-cf87-4fe5-8965-27f0115f9de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train labels = test labels\n",
            "Number of documents in the corpus: 3916\n"
          ]
        }
      ],
      "source": [
        "dataset_name = \"VNTC_khoahoc\"\n",
        "path = ['/content/drive/MyDrive/VNTC_khoahoc/Train_Full/', '/content/drive/MyDrive/VNTC_khoahoc/Test_Full/']\n",
        "\n",
        "if os.listdir(path[0]) == os.listdir(path[1]):\n",
        "    folder_list = [os.listdir(path[0]), os.listdir(path[1])]\n",
        "    print(\"train labels = test labels\")\n",
        "else:\n",
        "    print(\"train labels differ from test labels\")\n",
        "\n",
        "doc_num = 0\n",
        "sentences_list = []\n",
        "meta_data_list = []\n",
        "for i in range(2):\n",
        "    for folder_name in folder_list[i]:\n",
        "        folder_path = path[i] + folder_name\n",
        "        if folder_name[0] != \".\":\n",
        "            for file_name in glob.glob(os.path.join(folder_path, '*.txt')):\n",
        "                # Read the file content into f\n",
        "                f = codecs.open(file_name, 'br')\n",
        "                # Convert the data to UTF-16 format for Vietnamese text\n",
        "                file_content = (f.read().decode(\"utf-16\")).replace(\"\\r\\n\", \" \")\n",
        "                sentences_list.append(file_content.strip())\n",
        "                f.close\n",
        "                # Count the number of documents\n",
        "                doc_num += 1\n",
        "\n",
        "#### YOUR CODE HERE ####\n",
        "\n",
        "# Write sentences_list to a file (dataset_name.txt)\n",
        "with open(f'{dataset_name}.txt', 'w', encoding = 'utf-8') as out_file:\n",
        "  for sentence in sentences_list:\n",
        "    out_file.write(f\"{sentence}\\n\")\n",
        "\n",
        "# Number of documents in the corpus\n",
        "print(f\"Number of documents in the corpus: {doc_num}\")\n",
        "#### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bJuz5TFMGqB"
      },
      "source": [
        "## Question 2: Write Preprocessing Functions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KXHcDpuxKd0"
      },
      "source": [
        "### Question 2.1: Write a Function to Clean Text\n",
        "Hint:\n",
        "- The text should only retain the following characters: aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?\\'\\\n",
        "- Then trim the whitespace in the input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "id": "k8hIglDXxKd0"
      },
      "outputs": [],
      "source": [
        "def clean_str(string):\n",
        "    #### YOUR CODE HERE ####\n",
        "    # defined the allowed characters\n",
        "    allowed_chars = r\"aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?'\\\"\"\n",
        "\n",
        "    # allowed_chars_escaped = re.escape(allowed_chars)\n",
        "    # retain only the allowed characters\n",
        "    cleaned_text = re.sub(f\"[^{allowed_chars}]\", ' ', string)\n",
        "\n",
        "    # trim the whitespace in the input text\n",
        "    return cleaned_text.strip()\n",
        "    #### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KfXstqAxKd1"
      },
      "source": [
        "### Question 2.2: Write a Function to Convert Text to Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {
        "id": "KRwgVjxhxKd1"
      },
      "outputs": [],
      "source": [
        "# make all text lowercase\n",
        "def text_lowercase(string):\n",
        "\n",
        "    #### YOUR CODE HERE ####\n",
        "\n",
        "    # convert the entire string to lowercase\n",
        "    text = string.lower()\n",
        "\n",
        "    return text\n",
        "    #### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYM_GO_5xKd2"
      },
      "source": [
        "### Question 2.3: Tokenize Words\n",
        "Hint: Use the `word_tokenize()` function imported above with two parameters: `strings` and `format=\"text\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {
        "id": "pty34NwyxKd2"
      },
      "outputs": [],
      "source": [
        "def tokenize(strings):\n",
        "    #### YOUR CODE HERE ####\n",
        "\n",
        "    # tokenizing the text with format = \"text\"\n",
        "    tokenized_text = word_tokenize(strings, format=\"text\")\n",
        "\n",
        "    return tokenized_text\n",
        "    #### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gQGmL4gxKd2"
      },
      "source": [
        "### Question 2.4: Remove Stop Words\n",
        "To remove stop words, we use a list of Vietnamese stop words stored in the file `./vietnamese-stopwords.txt`. Complete the following program:\n",
        "- Check each word in the text (`strings`). If a word is not in the stop words list, add it to `doc_words`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {
        "id": "aqStv2rPxKd3"
      },
      "outputs": [],
      "source": [
        "stopwords_file = '/vietnamese-stopwords.txt'\n",
        "def remove_stopwords(strings):\n",
        "    #### YOUR CODE HERE ####\n",
        "    # load stop words\n",
        "    with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
        "      stop_words = set(f.read().splitlines())\n",
        "    doc_words = strings.split()\n",
        "\n",
        "    filter_words = [word for word in doc_words if word not in stop_words]\n",
        "    # doc_words = []\n",
        "\n",
        "    # # iterate through each tokenized word\n",
        "    # for word in strings:\n",
        "    #   if word not in stop_words:\n",
        "    #     doc_words.append(word)\n",
        "\n",
        "    return ' '.join(filter_words)\n",
        "    #### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUNOKigIxKd4"
      },
      "source": [
        "## Question 2.5: Build a Preprocessing Function\n",
        "Hint: Call the functions `clean_str`, `text_lowercase`, `tokenize`, and `remove_stopwords` in order, then return the result from the function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {
        "id": "_vd-el91xKd_"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(strings):\n",
        "    #### YOUR CODE HERE ####\n",
        "    text = clean_str(strings)\n",
        "    text = text_lowercase(text)\n",
        "    text = tokenize(text)\n",
        "    text = remove_stopwords(text)\n",
        "\n",
        "    return text\n",
        "    #### END YOUR CODE #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BGOqa1mxKeA"
      },
      "source": [
        "## Question 3: Perform Preprocessing\n",
        "Now, we will read the corpus from the file created in Question 1. After that, we will call the preprocessing function for each document in the corpus.\n",
        "\n",
        "Hint: Call the `text_preprocessing()` function with `doc_content` as the input parameter and save the result in the variable `temp1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {
        "id": "eBaBL79IMGqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f28522-0ef1-4965-b7c7-e815e46cf962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "length of clean_docs =  3916\n",
            "clean_docs[0]:\n",
            "trung_quốc phóng_vệ_tinh nghiên_cứu khoa_học 27 4 , trung_quốc phóng thành_công vệ_tinh nghiên_cứu quỹ_đạo , phóng_vệ_tinh đầu_tiên trung_quốc vệ_tinh nghiên_cứu khoa_học trung_quốc đồng_thời , thành_công món quà nhân_dịp kỷ_niệm 50 chương_trình phi_hành_vũ_trụ trung_quốc 6 48 phút 27 4 , trung_quốc phóng vệ_tinh nghiên_cứu long march 4 b tên_lửa trung_tâm phóng_vệ tinh_taiyuan quỹ đạo_vệ_tinh nghiên_cứu 2,7 sử_dụng thí_nghiệm khoa_học , khảo_sát tài_nguyên đất , đánh_giá cây_trồng ngăn_ngừa , thảm_họa trái_đất quan_chức viện nghiên_cứu công_nghệ phi_hành_vũ_trụ thượng_hải trung_quốc thành_công khởi_đầu phóng_vệ_tinh đầu_tiên đồng_thời , món quà tặng lễ kỷ_niệm 50 chương_trình phi_hành_vũ_trụ trung_quốc tiết_lộ , , trung_quốc phóng vệ_tinh liên_lạc vệ_tinh_thí_nghiệm khoa_học trung_quốc liên_tục phóng vệ_tinh_thám_hiểm_vũ_trụ 2003 trung_quốc thành_công đi vũ_trụ\n"
          ]
        }
      ],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "clean_docs = []\n",
        "file_path = f'{dataset_name}.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "  for line in file:\n",
        "    # remove unnecessary spaces\n",
        "    doc_content = line.strip()\n",
        "    # call preprocessing function for each document in the corpus\n",
        "    temp1 = text_preprocessing(doc_content)\n",
        "    # append to the clean_docs list\n",
        "    clean_docs.append(temp1)\n",
        "\n",
        "#### END YOUR CODE #####\n",
        "print(\"\\nlength of clean_docs = \", len(clean_docs))\n",
        "print('clean_docs[0]:\\n' + clean_docs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFhai6BwxKeB"
      },
      "source": [
        "## Question 4: Save Preprocessed Data\n",
        "Hint: Save the preprocessed data to a file named `dataset_name + '.clean.txt'`, where each document is written on a separate line.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {
        "id": "xfHmSiRrxKeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa13b96-7f43-4b27-d46c-9615e42f35f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed data: VNTC_khoahoc.clean.txt\n"
          ]
        }
      ],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# define the result name\n",
        "result = f\"{dataset_name}.clean.txt\"\n",
        "\n",
        "# open the file and write on a separate line\n",
        "with open(result, 'w', encoding='utf-8') as f:\n",
        "  for doc in clean_docs:\n",
        "    f.write(doc + '\\n')\n",
        "\n",
        "# file result\n",
        "print(f\"Preprocessed data: {result}\")\n",
        "#### YOUR CODE HERE ####"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "labNLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}