{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9kW2CvEXqt94"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import re\n",
        "import nltk\n",
        "import math\n",
        "import contractions\n",
        "from collections import defaultdict\n",
        "from difflib import get_close_matches, SequenceMatcher"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('punkt')\n",
        " nltk.download('punkt_tab')\n",
        " !pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snoxCkWsgSNc",
        "outputId": "9c5e5332-a3ce-46ed-b222-0a869b7b4744"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Downloading"
      ],
      "metadata": {
        "id": "AbhtxTnJhqHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/tedtalk.txt\"\n",
        "with open(file_path, 'r', encoding = 'utf-8') as f:\n",
        "  data = f.read()"
      ],
      "metadata": {
        "id": "78BNdueEe7uk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "MrxlQ2aff4I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing data\n",
        "def preprocess_text(data):\n",
        "  # convert to lowercase\n",
        "  text = data.lower()\n",
        "  # separate text into sentences\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  processed_stc = []\n",
        "\n",
        "  for sentence in sentences:\n",
        "    # expand contractions\n",
        "    sentence = contractions.fix(sentence)\n",
        "    # remove punctuation except ., !, ?\n",
        "    sentence = re.sub(r\"[^a-z0-9.!?]\", \" \", sentence)\n",
        "    # remove multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "    # tokenize using nltk\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "    # add <s> at the beginning and </s> at the end for sentence boundary\n",
        "    processed_stc.append([\"<s>\"] + tokens + [\"</s>\"])\n",
        "\n",
        "  return processed_stc\n",
        "\n",
        "# processed_data\n",
        "processed_data = preprocess_text(data)"
      ],
      "metadata": {
        "id": "c0Th7Zsbf7TC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Improve the model by using interpolation smoothing with the \"Stupid Backoff\" method"
      ],
      "metadata": {
        "id": "z43J1-TKe4Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StupidBackoffModel:\n",
        "  # n (int): Highest n-gram order\n",
        "  # alpha (float): Backoff discount factor (typically 0.4 as Brants et al., 2007)\n",
        "    def __init__(self, n=3, alpha=0.4):\n",
        "        self.n = n\n",
        "        self.alpha = alpha  # Discount factor for backoff\n",
        "        # Stores n-gram counts: index 0=unigrams, 1=bigrams, 2=trigrams\n",
        "        self.ngrams = [defaultdict(int) for _ in range(n)]\n",
        "        self.vocab = set()  # Unique words in the corpus\n",
        "        self.total_words = 0  # Total tokens\n",
        "\n",
        "    # train the model\n",
        "    def train(self, processed_data):\n",
        "        for sentence in processed_data:\n",
        "            self.total_words += len(sentence)\n",
        "            for i in range(len(sentence)):\n",
        "                word = sentence[i]\n",
        "                # Count unigrams (order 0)\n",
        "                self.ngrams[0][(word,)] += 1\n",
        "                self.vocab.add(word)\n",
        "                # Count bigrams (order 1) if there is a previous word\n",
        "                if i >= 1:\n",
        "                    bigram = tuple(sentence[i-1:i+1])\n",
        "                    self.ngrams[1][bigram] += 1\n",
        "                # Count trigrams (order 2) if there are two previous words\n",
        "                if i >= 2:\n",
        "                    trigram = tuple(sentence[i-2:i+1])\n",
        "                    self.ngrams[2][trigram] += 1\n",
        "\n",
        "    # Computes the score for a candidate word given a context using Stupid Backoff\n",
        "    def score(self, context, candidate):\n",
        "        current_context = list(context)\n",
        "        accumulated_alpha = 1.0\n",
        "\n",
        "        # check trigram to unigram\n",
        "        max_order = min(len(current_context), self.n - 1)\n",
        "        for order in range(max_order, 0, -1):\n",
        "            ngram = tuple(current_context[-order:] + [candidate])\n",
        "            count = self.ngrams[order].get(ngram, 0)\n",
        "\n",
        "            if count > 0:\n",
        "                prefix = tuple(current_context[-order:])\n",
        "                prefix_count = self.ngrams[order-1].get(prefix, 0) if order > 0 else self.total_words\n",
        "                if prefix_count == 0:\n",
        "                    continue\n",
        "                return accumulated_alpha * (count / prefix_count)\n",
        "\n",
        "            # Backoff: reduce context and accumulate alpha\n",
        "            current_context = current_context[1:]\n",
        "            accumulated_alpha *= self.alpha\n",
        "\n",
        "        # Fallback to unigram\n",
        "        unigram_count = self.ngrams[0].get((candidate,), 0)\n",
        "        return accumulated_alpha * (unigram_count / self.total_words)\n"
      ],
      "metadata": {
        "id": "bxM7HdsokO3-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ],
      "metadata": {
        "id": "TAeU-ozspjwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train model\n",
        "model = StupidBackoffModel(n=3, alpha=0.4)\n",
        "model.train(processed_data)"
      ],
      "metadata": {
        "id": "3rWryqOmpmEQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Compare with the results from In Class Exercise"
      ],
      "metadata": {
        "id": "aFWylJ4mijtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare with the results from in-class exercise using sentence probability and perplexity. The results from in-class exercise:\n",
        "\n",
        "1-gram probability: 1.3281553436964184e-17, 1-gram perplexity: 128.70454711879646\n",
        "\n",
        "2-gram probability: 5.088333895898929e-16, 2-gram perplexity: 81.59772486118717\n",
        "\n",
        "3-gram probability: 1.8867762518029128e-16, 3-gram perplexity: 92.37085208501146"
      ],
      "metadata": {
        "id": "cGMP6M14sCkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate probability\n",
        "def calculate_sentence_probability(model, processed_sentence):\n",
        "    tokens = processed_sentence\n",
        "    probability = 1.0\n",
        "\n",
        "    # Iterate from the starting position with sufficient context (n-1 words)\n",
        "    for i in range(model.n - 1, len(tokens)):\n",
        "        context = tokens[i - (model.n - 1):i]  # context\n",
        "        word = tokens[i]\n",
        "        prob = model.score(context, word)\n",
        "        probability *= prob if prob > 0 else 1e-10  # avoid zero\n",
        "\n",
        "    return probability"
      ],
      "metadata": {
        "id": "5G9prJjWsZoO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate perplexity\n",
        "def calculate_perplexity(model, processed_sentence):\n",
        "    m = len(processed_sentence)\n",
        "    prob = calculate_sentence_probability(model, processed_sentence)\n",
        "    return math.pow(prob, -1/m)"
      ],
      "metadata": {
        "id": "7sWNOQfMe6rq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example sentence\n",
        "ex = \"I have to go to school\"\n",
        "ex = preprocess_text(ex)\n",
        "ex = [word for sentence in ex for word in sentence]\n",
        "\n",
        "print(ex)\n",
        "\n",
        "prob_trigram = calculate_sentence_probability(model, ex)\n",
        "perplexity_trigram = calculate_perplexity(model, ex)\n",
        "print(f\"[Trigram Stupid Backoff]\")\n",
        "print(f\"Probability: {prob_trigram:.10f}\")\n",
        "print(f\"Perplexity: {perplexity_trigram:.2f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqoLcjptwgbv",
        "outputId": "8919a224-77f7-4ead-fdd2-363e83796b25"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'i', 'have', 'to', 'go', 'to', 'school', '</s>']\n",
            "[Trigram Stupid Backoff]\n",
            "Probability: 0.0000000156\n",
            "Perplexity: 9.46\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparsion:\n",
        "\n",
        "#### **1. Probability Comparison**  \n",
        "- **Laplace Smoothing:**  \n",
        "  - 1-gram: $$ 1.33 \\times 10^{-17} $$  \n",
        "  - 2-gram: $$ 5.09 \\times 10^{-16} $$\n",
        "  - 3-gram: $$ 1.89 \\times 10^{-16} $$  \n",
        "- **Stupid Backoff (Trigram-based):**  \n",
        "  - Probability: $$ 1.56 \\times 10^{-8} $$\n",
        "\n",
        "The probability from Stupid Backoff is higher than all Laplace-based probabilities. This happens because Laplace smoothing assigns nonzero probability to unseen n-grams, which tends to over-smooth the distribution, leading to lower overall probability estimates. In contrast, Stupid Backoff prioritizes longer n-grams first and only backs off when necessary, leading to relatively higher probability estimates.\n",
        "\n",
        "#### **2. Perplexity Comparison**  \n",
        "- **Laplace Smoothing:**  \n",
        "  - 1-gram: 128.70  \n",
        "  - 2-gram: 81.60  \n",
        "  - 3-gram: 92.37  \n",
        "- **Stupid Backoff (Trigram-based):**  \n",
        "  - Perplexity: 9.46  \n",
        "\n",
        "The Stupid Backoff model results in significantly lower perplexity than all Laplace smoothing models. Since perplexity measures how well the model predicts the test sentence, a lower perplexity means a better predictive model. Stupid Backoff generally performs better because it utilizes higher-order n-grams more effectively rather than applying uniform smoothing across all n-grams.\n"
      ],
      "metadata": {
        "id": "n8TpmbK28CxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c. Generate the next words for a given word sequence"
      ],
      "metadata": {
        "id": "46_H2k0lihZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the top-k next words for a given context using the Stupid Backoff model\n",
        "def generate_next_words(model, context, top_k=5): # specify top words = 5\n",
        "    candidates = []\n",
        "    for word in model.vocab:\n",
        "        score = model.score(context, word)\n",
        "        candidates.append((word, score))\n",
        "\n",
        "    # Sort by score (descending) and return top-k\n",
        "    candidates.sort(key=lambda x: -x[1])\n",
        "    return candidates[:top_k]"
      ],
      "metadata": {
        "id": "trWCjen_8g99"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "context = [\"the\", \"speaker\", \"is\"]\n",
        "next_words = generate_next_words(model, context, top_k=5)\n",
        "print(f\"Next words after '{' '.join(context)}':\")\n",
        "for word, score in next_words:\n",
        "    print(f\"- {word}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OBc6tqX9s1W",
        "outputId": "c353fe89-1f9c-44b9-a928-c3c1224436b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next words after 'the speaker is':\n",
            "- to: 0.2222\n",
            "- going: 0.2222\n",
            "- trying: 0.1111\n",
            "- saying: 0.1111\n",
            "- an: 0.1111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d. Combine with a function that calculates the distance between words to predict the correct word for a misspelled word position"
      ],
      "metadata": {
        "id": "qWiQaAAZinpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct a misspelled word in a sentence using similarity and language model scores\n",
        "def correct_spelling(model, sentence, error_position, num_candidates=3, cutoff=0.6):\n",
        "\n",
        "    tokens = preprocess_text(sentence)\n",
        "\n",
        "    tokens = [word for sentence in tokens for word in sentence]\n",
        "    tokens = [word for word in tokens if word not in {\"<s>\", \"</s>\"}]\n",
        "\n",
        "    misspelled_word = tokens[error_position]\n",
        "\n",
        "    # generate spelling candidates using difflib\n",
        "    candidates = get_close_matches(misspelled_word, model.vocab, n=num_candidates, cutoff=cutoff)\n",
        "\n",
        "    if not candidates:\n",
        "        return sentence  # no correction possible\n",
        "\n",
        "    # get context\n",
        "    context_start = max(0, error_position - (model.n - 1))\n",
        "    context = tokens[context_start:error_position]\n",
        "\n",
        "    # using both similarity and LM score to find candidate\n",
        "    best_candidate = None\n",
        "    best_score = -1\n",
        "    for candidate in candidates:\n",
        "        # Language model score\n",
        "        lm_score = model.score(context, candidate)\n",
        "        # Similarity score (0-1)\n",
        "        similarity = SequenceMatcher(None, misspelled_word, candidate).ratio()\n",
        "        # Combined score\n",
        "        combined_score = lm_score * similarity\n",
        "        if combined_score > best_score:\n",
        "            best_score = combined_score\n",
        "            best_candidate = candidate\n",
        "\n",
        "    # replace the misspelled word\n",
        "    tokens[error_position] = best_candidate\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "TB2NdXSG94cM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example\n",
        "ex = \"I hae to go to schol\"\n",
        "# start with 0\n",
        "corrected = correct_spelling(model, ex, error_position=1)  # Correct \"hae\" → \"have\"\n",
        "corrected = correct_spelling(model, corrected, error_position=5)  # Correct \"schol\" → \"school\"\n",
        "print(f\"Original: {ex}\")\n",
        "print(f\"Corrected: {corrected}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMRgdly7-s2o",
        "outputId": "954b7234-053e-474f-e8b1-8719f68e031e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: I hae to go to schol\n",
            "Corrected: i have to go to school\n"
          ]
        }
      ]
    }
  ]
}